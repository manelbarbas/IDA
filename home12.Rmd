---
title: "Homework 1.2 Report"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

###### Monika Wysoczañska, 180817
###### Manuel Barbas, 180832
###### Diogo Oliveira, 180832\

```{r setup, include=FALSE}
library("moments")
library(pastecs)
library(ggplot2)
library(car)
library(MVN)
library(mvoutlier)

```

## Cars Dataset
```{r, include=FALSE}
## Intro
## loading the data and the first glance
cars<-read.table("cars-PCA.txt")

summary(cars)
str(cars)

# origin is a factor, as well as year
cars$V8=factor(cars$V8)
model_y = sort(unique(cars$V7))
cars$V7=factor(cars$V7, labels = model_y)
```

### Description
Cars dataset consists of 99 observations of different car models. Each one of them is described by 9 different variables, 4 categorical: cylinders, car name, model year, origin, and 5 quantitative: engine displacement, horsepower, weight, acceleration, mpg.

### Univariate analysis





### Bivariate analysis
```{r include = FALSE}
## 1.2.b) Bivariate Analysis

# histogram of cost variable

## 1.2 b)
# first we check univariate normality for chosen variables
hist(cars$V4)
hist(cars$V5)
```

We start with basic scatterplot to see the distribution of the data for the two chosen variables, which are:  
1. Horsepower (V4)  
2. Weight (V5)


```{r, echo=FALSE}

# scatter plot just to have a look on overall bivariate data distribution
p1 <- ggplot(cars, aes(x=V5, y=V4)) + geom_point()

p1 + labs(title="Horsepower by weight", y="Horsepower", x="Weight")

```

We observe the general positive relationship between the two variables, but by the given plot we cannot say anything about bivariate normality yet. What we can conclude by now, is we'll probably be dealing with some outliers in the data. 
First of all we perform Mardia's multivariate normality test. 

```{r, echo=FALSE}
# bivariate normality
mvn(cbind(cars$V4,cars$V5), mvnTest="mardia", multivariatePlot="qq")
```

As we can see both variables fail the univariate normality test. They also fail bivariate normality test, because of the skewness. 
We try to apply the Box-Cox transformation so as to improve bivariate normality, with the parameters given below:

```{r, echo=FALSE}
powerTransform(cbind(cars$V4,cars$V5))
bivT=bcPower(cbind(cars$V4,cars$V5), c(-0.079,0.46))
```

After applying the transormation we conduct bivariate normality analysis the same way as before.
```{r, echo=FALSE}
afterTrans<-mvn(bivT, mvnTest="mardia", multivariatePlot="qq")
afterTrans$multivariateNormality
```
As we can see, the normality has been improved as data after Box-Cox transform passes both tests, for kurtosis as well as for skewness.

#### Outliers detection

Another thing we want to conduct during our bivariate analysis is the outliers detection. To achieve this we use 'mvoutlier' package. Firstly, we apply 'pcout' method on the original dataset.

```{r, echo=FALSE}

outs=pcout(cbind(cars$V4,cars$V5), makeplot=TRUE)
outliersOrg<-which(outs$wfinal01==0)
length(outliersOrg)
```
We detected 18 outliers in the original dataset based on the bivariate analysis, and it 
We also applied the same method for the transformed dataset.

```{r, echo=FALSE}
# outliers after transform 
outsT=pcout(bivT, makeplot=TRUE)
length(which(outsT$wfinal01==0))
```

The dataset meeting bivariate normality criteria for chosen variables includes only 5 outliers, which are:
```{r, echo=FALSE}
cars[which(outsT$wfinal01==0),"V9"]
```
```{r, include=FALSE}

carsNoOut<-cars[-outliersOrg,]
mvn(cbind(carsNoOut$V4,carsNoOut$V5), mvnTest="mardia", multivariatePlot="qq")
```
As we analyzed, more than 18% of the original dataset has been classified as outliers. Depends on the type of each outlier and obviously the main objectivity of our analysis, sometimes we may consider outlier removal. In case of our dataset this is not an option, since it consists of only 99 observations.  
After normality improvement we qualified about 5% samples as the outliers, and none of them seems to be a typing mistake. They should be taken into account in further analysis.


### 1.2.2 Permutation test
In this case we want to see relationship between the size of the bill and percent tip and we will use the permutation test to help us.
We started to combine the values of both variables using the c function.

```{r, include=FALSE}
#R -> number of simulations
nSimulations <- 10000

#vector of combined values
combined_scores <- c(RestaurantTips$Bill,RestaurantTips$PctTip)

#Represent each variable value with a label (1 to Bill and 0 to PctTip)
labels <- c(rep(1,length(RestaurantTips$Bill)),rep(0,length(RestaurantTips$PctTip)))

#Initialize the diffs and nR vector
diffs <- rep(NA,nSimulations)
nR <- rep(NA,nSimulations)
```
**##Results:**
Observed correlation between the variables (**Bill** and **PctTip**): 
```{r, include=FALSE}
cor.test(RestaurantTips$Bill,RestaurantTips$PctTip, method ="pearson")

#Correlation value of the variables
rObs <- cor(RestaurantTips$Bill,RestaurantTips$PctTip, method ="pearson")
rObs
#permutation test
for(i in 1:nSimulations){
  shuffled_labels <- sample(labels, replace = FALSE)
  diffs[i]<- mean(combined_scores[shuffled_labels == 1]) - mean(combined_scores[shuffled_labels == 0])
  nR[i] <- cor(combined_scores[shuffled_labels == 1],combined_scores[shuffled_labels == 0])
}
```
After calculating the value of the correlation between the two variables we started to execute the permutation test and for that we have calculated the correlation values for each step between the variables mentioned before. We ran the test 10000 times to see how the correlation values were changing. To help us understanding the real relation we have calculated also the difference of the mean values for each step. We had the following result:
```{r, include=FALSE}
#Histogram
hist(diffs)
```
The following histogram represents the different values of the correlation between the variables after the permutation test
```{r, include=FALSE}
hist(nR)
```
Using the vector of the correlation values calculated before was possible to do the test described in the exercise sheet (upper-tail test). The upper-tail test is a statistical test in which the critical area of a distribution is one-sided so that it is either greater than or less than a certain value, but not both. 
We reach the following value:
```{r, include=FALSE}
#upper-tail test p-value
sum(nR>rObs)/nSimulations
```
**##Analysis: **
With only the first correlation value obtained (0.1352976) we can conclude that the two variables have a weak association. After executing the permutation test (we can see in the difference histogram) that the difference of the mean value of each variable falls over most of the time (not always) near zero. With this we assume that the values are frequently similar, with some exceptions. 
When we look to the correlation values obtained do the test we can see that the values never exceed the value **0.4** (positive way) neither **-0.2** (negative way). Supported with the following figure, the values obtained with the permutation test infer that the “Strength of Association” is small. This means that both variables are poorly linear related.

![](/IDA/corre.png)

Our final test was the upper-tailed test and we obtained the p-value 0.4347. We took the vector of correlation values obtained and counted the number of values greater than the original correlation value (rObs), over the number of simulations (10000). 
With this p-value (relatively large) we assume that there is a weak evidence against the null hypothesis, so we fail to reject the null hypothesis.
 Said this, we conclude that the value of the tip isn’t related to how much a customer spends on a bill.

