---
title: "Homework 1.2 Report"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

###### Monika Wysoczañska, 180817
###### Manuel Barbas, 180832
###### Diogo Oliveira, 180832\

```{r setup, include=FALSE}
library("moments")
library(pastecs)
library(ggplot2)
library(car)
library(MVN)
library(mvoutlier)

```

## Cars Dataset
```{r, include=FALSE}
## Intro
## loading the data and the first glance
cars<-read.table("cars-PCA.txt")

summary(cars)
str(cars)

# origin is a factor, as well as year
cars$V8=factor(cars$V8)
model_y = sort(unique(cars$V7))
cars$V7=factor(cars$V7, labels = model_y)
```

### Description
Cars dataset consists of 99 observations of different car models. Each one of them is described by 9 different variables, 4 categorical: cylinders, car name, model year, origin, and 5 quantitative: engine displacement, horsepower, weight, acceleration, mpg.

### Univariate analysis





### Bivariate analysis
```{r include = FALSE}
## 1.2.b) Bivariate Analysis

# histogram of cost variable

## 1.2 b)
# first we check univariate normality for chosen variables
hist(cars$V4)
hist(cars$V5)
```

We start with basic scatterplot to see the distribution of the data for the two chosen variables, which are:  
1. Horsepower (V4)  
2. Weight (V5)


```{r, echo=FALSE}

# scatter plot just to have a look on overall bivariate data distribution
p1 <- ggplot(cars, aes(x=V5, y=V4)) + geom_point()

p1 + labs(title="Horsepower by weight", y="Horsepower", x="Weight")

```

We observe the general positive relationship between the two variables, but by the given plot we cannot say anything about bivariate normality yet. What we can conclude by now, is we'll probably be dealing with some outliers in the data. 
First of all we perform Mardia's multivariate normality test. 

```{r, echo=FALSE}
# bivariate normality
mvn(cbind(cars$V4,cars$V5), mvnTest="mardia", multivariatePlot="qq")
```

As we can see both variables fail the univariate normality test. They also fail bivariate normality test, because of the skewness. 
We try to apply the Box-Cox transformation so as to improve bivariate normality, with the parameters given below:

```{r, echo=FALSE}
powerTransform(cbind(cars$V4,cars$V5))
bivT=bcPower(cbind(cars$V4,cars$V5), c(-0.079,0.46))
```

After applying the transormation we conduct bivariate normality analysis the same way as before.
```{r, echo=FALSE}
afterTrans<-mvn(bivT, mvnTest="mardia", multivariatePlot="qq")
afterTrans$multivariateNormality
```
As we can see, the normality has been improved as data after Box-Cox transform passes both tests, for kurtosis as well as for skewness.

#### Outliers detection

Another thing we want to conduct during our bivariate analysis is the outliers detection. To achieve this we use 'mvoutlier' package. Firstly, we apply 'pcout' method on the original dataset.

```{r, echo=FALSE}

outs=pcout(cbind(cars$V4,cars$V5), makeplot=TRUE)
outliersOrg<-which(outs$wfinal01==0)
length(outliersOrg)
```
We detected 18 outliers in the original dataset based on the bivariate analysis, and it 
We also applied the same method for the transformed dataset.

```{r, echo=FALSE}
# outliers after transform 
outsT=pcout(bivT, makeplot=TRUE)
length(which(outsT$wfinal01==0))
```

The dataset meeting bivariate normality criteria for chosen variables includes only 5 outliers, which are:
```{r, echo=FALSE}
cars[which(outsT$wfinal01==0),"V9"]
```
```{r, include=FALSE}

carsNoOut<-cars[-outliersOrg,]
mvn(cbind(carsNoOut$V4,carsNoOut$V5), mvnTest="mardia", multivariatePlot="qq")
```
As we analyzed, more than 18% of the original dataset has been classified as outliers. Depends on the type of each outlier and obviously the main objectivity of our analysis, sometimes we may consider outlier removal. In case of our dataset this is not an option, since it consists of only 99 observations.  
After normality improvement we qualified about 5% samples as the outliers, and none of them seems to be a typing mistake. They should be taken into account in further analysis.


## 1.2.2 Permutation test

#R -> number of simulations
nSimulations <- 10000

#vector of combined values
combined_scores <- c(RestaurantTips$Bill,RestaurantTips$PctTip)

#Represent each variable value with a label (1 to Bill and 0 to PctTip)
labels <- c(rep(1,length(RestaurantTips$Bill)),rep(0,length(RestaurantTips$PctTip)))

#Initialize the diffs and nR vector
diffs <- rep(NA,nSimulations)
nR <- rep(NA,nSimulations)

#Correlation value of the variables
rObs <- cor(RestaurantTips$Bill,RestaurantTips$PctTip, method ="pearson")
cor.test(RestaurantTips$Bill,RestaurantTips$PctTip, method ="pearson")

#permutation test
for(i in 1:nSimulations){
  shuffled_labels <- sample(labels, replace = FALSE)
  diffs[i]<- mean(combined_scores[shuffled_labels == 1]) - mean(combined_scores[shuffled_labels == 0])
  nR[i] <- cor(combined_scores[shuffled_labels == 1],combined_scores[shuffled_labels == 0])
}

#Histogram
hist(nR)
hist(diffs)

#upper-tail test p-value
sum(nR>rObs)/nSimulations
